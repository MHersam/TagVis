{"graph":[],"links":[{"id":"l59","source":3,"target":9,"distance":400,"tags":[],"correspondenceRatio":0},{"id":"l112","source":7,"target":8,"distance":400,"tags":[],"correspondenceRatio":0},{"id":"l183","source":15,"target":19,"distance":400,"tags":[],"correspondenceRatio":0},{"id":"l179","source":14,"target":19,"distance":400,"tags":[],"correspondenceRatio":0},{"id":"l8","source":0,"target":9,"distance":400,"tags":[],"correspondenceRatio":0},{"id":"l178","source":14,"target":18,"distance":400,"tags":[],"correspondenceRatio":0},{"id":"l177","source":14,"target":17,"distance":400,"tags":[],"correspondenceRatio":0},{"id":"l13","source":0,"target":14,"distance":400,"tags":[],"correspondenceRatio":0},{"id":"l176","source":14,"target":16,"distance":400,"tags":[],"correspondenceRatio":0},{"id":"l175","source":14,"target":15,"distance":400,"tags":[],"correspondenceRatio":0},{"id":"l20","source":1,"target":3,"distance":400,"tags":[],"correspondenceRatio":0},{"id":"l21","source":1,"target":4,"distance":400,"tags":[],"correspondenceRatio":0},{"id":"l23","source":1,"target":6,"distance":400,"tags":[],"correspondenceRatio":0},{"id":"l25","source":1,"target":8,"distance":400,"tags":[],"correspondenceRatio":0},{"id":"l26","source":1,"target":9,"distance":400,"tags":[],"correspondenceRatio":0},{"id":"l27","source":1,"target":10,"distance":400,"tags":[],"correspondenceRatio":0},{"id":"l29","source":1,"target":12,"distance":400,"tags":[],"correspondenceRatio":0},{"id":"l31","source":1,"target":14,"distance":400,"tags":[],"correspondenceRatio":0},{"id":"l32","source":1,"target":15,"distance":400,"tags":[],"correspondenceRatio":0},{"id":"l33","source":1,"target":16,"distance":400,"tags":[],"correspondenceRatio":0},{"id":"l34","source":1,"target":17,"distance":400,"tags":[],"correspondenceRatio":0},{"id":"l35","source":1,"target":18,"distance":400,"tags":[],"correspondenceRatio":0},{"id":"l36","source":1,"target":19,"distance":400,"tags":[],"correspondenceRatio":0},{"id":"l37","source":2,"target":3,"distance":400,"tags":[],"correspondenceRatio":0},{"id":"l38","source":2,"target":4,"distance":400,"tags":[],"correspondenceRatio":0},{"id":"l169","source":13,"target":14,"distance":400,"tags":[],"correspondenceRatio":0},{"id":"l40","source":2,"target":6,"distance":400,"tags":[],"correspondenceRatio":0},{"id":"l165","source":12,"target":16,"distance":400,"tags":[],"correspondenceRatio":0},{"id":"l164","source":12,"target":15,"distance":400,"tags":[],"correspondenceRatio":0},{"id":"l43","source":2,"target":9,"distance":400,"tags":[],"correspondenceRatio":0},{"id":"l163","source":12,"target":14,"distance":400,"tags":[],"correspondenceRatio":0},{"id":"l158","source":11,"target":16,"distance":400,"tags":[],"correspondenceRatio":0},{"id":"l157","source":11,"target":15,"distance":400,"tags":[],"correspondenceRatio":0},{"id":"l48","source":2,"target":14,"distance":400,"tags":[],"correspondenceRatio":0},{"id":"l49","source":2,"target":15,"distance":400,"tags":[],"correspondenceRatio":0},{"id":"l50","source":2,"target":16,"distance":400,"tags":[],"correspondenceRatio":0},{"id":"l156","source":11,"target":14,"distance":400,"tags":[],"correspondenceRatio":0},{"id":"l150","source":10,"target":16,"distance":400,"tags":[],"correspondenceRatio":0},{"id":"l149","source":10,"target":15,"distance":400,"tags":[],"correspondenceRatio":0},{"id":"l148","source":10,"target":14,"distance":400,"tags":[],"correspondenceRatio":0},{"id":"l144","source":9,"target":19,"distance":400,"tags":[],"correspondenceRatio":0},{"id":"l57","source":3,"target":7,"distance":400,"tags":[],"correspondenceRatio":0},{"id":"l143","source":9,"target":18,"distance":400,"tags":[],"correspondenceRatio":0},{"id":"l186","source":16,"target":19,"distance":400,"tags":[],"correspondenceRatio":0},{"id":"l60","source":3,"target":10,"distance":400,"tags":[],"correspondenceRatio":0},{"id":"l61","source":3,"target":11,"distance":400,"tags":[],"correspondenceRatio":0},{"id":"l62","source":3,"target":12,"distance":400,"tags":[],"correspondenceRatio":0},{"id":"l142","source":9,"target":17,"distance":400,"tags":[],"correspondenceRatio":0},{"id":"l64","source":3,"target":14,"distance":400,"tags":[],"correspondenceRatio":0},{"id":"l141","source":9,"target":16,"distance":400,"tags":[],"correspondenceRatio":0},{"id":"l140","source":9,"target":15,"distance":400,"tags":[],"correspondenceRatio":0},{"id":"l138","source":9,"target":13,"distance":400,"tags":[],"correspondenceRatio":0},{"id":"l137","source":9,"target":12,"distance":400,"tags":[],"correspondenceRatio":0},{"id":"l69","source":3,"target":19,"distance":400,"tags":[],"correspondenceRatio":0},{"id":"l136","source":9,"target":11,"distance":400,"tags":[],"correspondenceRatio":0},{"id":"l72","source":4,"target":7,"distance":400,"tags":[],"correspondenceRatio":0},{"id":"l135","source":9,"target":10,"distance":400,"tags":[],"correspondenceRatio":0},{"id":"l74","source":4,"target":9,"distance":400,"tags":[],"correspondenceRatio":0},{"id":"l75","source":4,"target":10,"distance":400,"tags":[],"correspondenceRatio":0},{"id":"l76","source":4,"target":11,"distance":400,"tags":[],"correspondenceRatio":0},{"id":"l77","source":4,"target":12,"distance":400,"tags":[],"correspondenceRatio":0},{"id":"l79","source":4,"target":14,"distance":400,"tags":[],"correspondenceRatio":0},{"id":"l134","source":8,"target":19,"distance":400,"tags":[],"correspondenceRatio":0},{"id":"l129","source":8,"target":14,"distance":400,"tags":[],"correspondenceRatio":0},{"id":"l127","source":8,"target":12,"distance":400,"tags":[],"correspondenceRatio":0},{"id":"l125","source":8,"target":10,"distance":400,"tags":[],"correspondenceRatio":0},{"id":"l84","source":4,"target":19,"distance":400,"tags":[],"correspondenceRatio":0},{"id":"l124","source":8,"target":9,"distance":400,"tags":[],"correspondenceRatio":0},{"id":"l88","source":5,"target":9,"distance":400,"tags":[],"correspondenceRatio":0},{"id":"l123","source":7,"target":19,"distance":400,"tags":[],"correspondenceRatio":0},{"id":"l122","source":7,"target":18,"distance":400,"tags":[],"correspondenceRatio":0},{"id":"l93","source":5,"target":14,"distance":400,"tags":[],"correspondenceRatio":0},{"id":"l121","source":7,"target":17,"distance":400,"tags":[],"correspondenceRatio":0},{"id":"l120","source":7,"target":16,"distance":400,"tags":[],"correspondenceRatio":0},{"id":"l119","source":7,"target":15,"distance":400,"tags":[],"correspondenceRatio":0},{"id":"l99","source":6,"target":7,"distance":400,"tags":[],"correspondenceRatio":0},{"id":"l118","source":7,"target":14,"distance":400,"tags":[],"correspondenceRatio":0},{"id":"l101","source":6,"target":9,"distance":400,"tags":[],"correspondenceRatio":0},{"id":"l102","source":6,"target":10,"distance":400,"tags":[],"correspondenceRatio":0},{"id":"l103","source":6,"target":11,"distance":400,"tags":[],"correspondenceRatio":0},{"id":"l104","source":6,"target":12,"distance":400,"tags":[],"correspondenceRatio":0},{"id":"l106","source":6,"target":14,"distance":400,"tags":[],"correspondenceRatio":0},{"id":"l116","source":7,"target":12,"distance":400,"tags":[],"correspondenceRatio":0},{"id":"l114","source":7,"target":10,"distance":400,"tags":[],"correspondenceRatio":0},{"id":"l113","source":7,"target":9,"distance":400,"tags":[],"correspondenceRatio":0},{"id":"l111","source":6,"target":19,"distance":400,"tags":[],"correspondenceRatio":0},{"id":"l95","source":5,"target":16,"distance":100,"tags":["ToM"],"correspondenceRatio":0.14285714285714285},{"id":"l22","source":1,"target":5,"distance":100,"tags":["Games"],"correspondenceRatio":0.14285714285714285},{"id":"l6","source":0,"target":7,"distance":100,"tags":["Games"],"correspondenceRatio":0.14285714285714285},{"id":"l89","source":5,"target":10,"distance":100,"tags":["Multi-Agent"],"correspondenceRatio":0.14285714285714285},{"id":"l172","source":13,"target":17,"distance":100,"tags":["ToM"],"correspondenceRatio":0.14285714285714285},{"id":"l91","source":5,"target":12,"distance":100,"tags":["Multi-Agent"],"correspondenceRatio":0.14285714285714285},{"id":"l70","source":4,"target":5,"distance":100,"tags":["ToM"],"correspondenceRatio":0.14285714285714285},{"id":"l3","source":0,"target":4,"distance":100,"tags":["ToM"],"correspondenceRatio":0.14285714285714285},{"id":"l147","source":10,"target":13,"distance":100,"tags":["Multi-Agent"],"correspondenceRatio":0.14285714285714285},{"id":"l0","source":0,"target":1,"distance":100,"tags":["Games"],"correspondenceRatio":0.14285714285714285},{"id":"l96","source":5,"target":17,"distance":100,"tags":["ToM"],"correspondenceRatio":0.14285714285714285},{"id":"l15","source":0,"target":16,"distance":100,"tags":["ToM"],"correspondenceRatio":0.14285714285714285},{"id":"l98","source":5,"target":19,"distance":100,"tags":["Multi-Agent"],"correspondenceRatio":0.14285714285714285},{"id":"l171","source":13,"target":16,"distance":100,"tags":["ToM"],"correspondenceRatio":0.14285714285714285},{"id":"l16","source":0,"target":17,"distance":100,"tags":["ToM"],"correspondenceRatio":0.14285714285714285},{"id":"l30","source":1,"target":13,"distance":100,"tags":["Games"],"correspondenceRatio":0.14285714285714285},{"id":"l78","source":4,"target":13,"distance":100,"tags":["ToM"],"correspondenceRatio":0.14285714285714285},{"id":"l162","source":12,"target":13,"distance":100,"tags":["Multi-Agent"],"correspondenceRatio":0.14285714285714285},{"id":"l9","source":0,"target":10,"distance":100,"tags":["Multi-Agent"],"correspondenceRatio":0.14285714285714285},{"id":"l105","source":6,"target":13,"distance":100,"tags":["ToM"],"correspondenceRatio":0.14285714285714285},{"id":"l18","source":0,"target":19,"distance":100,"tags":["Multi-Agent"],"correspondenceRatio":0.14285714285714285},{"id":"l117","source":7,"target":13,"distance":100,"tags":["Games"],"correspondenceRatio":0.14285714285714285},{"id":"l174","source":13,"target":19,"distance":100,"tags":["Multi-Agent"],"correspondenceRatio":0.14285714285714285},{"id":"l5","source":0,"target":6,"distance":100,"tags":["ToM"],"correspondenceRatio":0.14285714285714285},{"id":"l11","source":0,"target":12,"distance":100,"tags":["Multi-Agent"],"correspondenceRatio":0.14285714285714285},{"id":"l85","source":5,"target":6,"distance":100,"tags":["ToM"],"correspondenceRatio":0.14285714285714285},{"id":"l86","source":5,"target":7,"distance":100,"tags":["Games"],"correspondenceRatio":0.14285714285714285},{"id":"l170","source":13,"target":15,"distance":100,"tags":["ToM"],"correspondenceRatio":0.16666666666666666},{"id":"l94","source":5,"target":15,"distance":100,"tags":["ToM"],"correspondenceRatio":0.16666666666666666},{"id":"l2","source":0,"target":3,"distance":100,"tags":["ToM"],"correspondenceRatio":0.16666666666666666},{"id":"l14","source":0,"target":15,"distance":100,"tags":["ToM"],"correspondenceRatio":0.16666666666666666},{"id":"l63","source":3,"target":13,"distance":100,"tags":["ToM"],"correspondenceRatio":0.16666666666666666},{"id":"l55","source":3,"target":5,"distance":100,"tags":["ToM"],"correspondenceRatio":0.16666666666666666},{"id":"l19","source":1,"target":2,"distance":100,"tags":["Games"],"correspondenceRatio":0.2},{"id":"l28","source":1,"target":11,"distance":100,"tags":["Games"],"correspondenceRatio":0.2},{"id":"l115","source":7,"target":11,"distance":100,"tags":["Games"],"correspondenceRatio":0.2},{"id":"l126","source":8,"target":11,"distance":100,"tags":["HCI"],"correspondenceRatio":0.2},{"id":"l41","source":2,"target":7,"distance":100,"tags":["Games"],"correspondenceRatio":0.2},{"id":"l42","source":2,"target":8,"distance":100,"tags":["HCI"],"correspondenceRatio":0.2},{"id":"l51","source":2,"target":17,"distance":100,"tags":["RL"],"correspondenceRatio":0.2},{"id":"l160","source":11,"target":18,"distance":100,"tags":["Multi-Agent"],"correspondenceRatio":0.2},{"id":"l159","source":11,"target":17,"distance":100,"tags":["RL"],"correspondenceRatio":0.2},{"id":"l52","source":2,"target":18,"distance":100,"tags":["Multi-Agent"],"correspondenceRatio":0.2},{"id":"l151","source":10,"target":17,"distance":100,"tags":["RL"],"correspondenceRatio":0.3333333333333333},{"id":"l7","source":0,"target":8,"distance":100,"tags":["HCI","ToM"],"correspondenceRatio":0.3333333333333333},{"id":"l131","source":8,"target":16,"distance":100,"tags":["ToM"],"correspondenceRatio":0.3333333333333333},{"id":"l132","source":8,"target":17,"distance":100,"tags":["ToM"],"correspondenceRatio":0.3333333333333333},{"id":"l133","source":8,"target":18,"distance":100,"tags":["ToM"],"correspondenceRatio":0.3333333333333333},{"id":"l100","source":6,"target":8,"distance":100,"tags":["ToM"],"correspondenceRatio":0.3333333333333333},{"id":"l73","source":4,"target":8,"distance":100,"tags":["ToM"],"correspondenceRatio":0.3333333333333333},{"id":"l71","source":4,"target":6,"distance":100,"tags":["ToM"],"correspondenceRatio":0.3333333333333333},{"id":"l17","source":0,"target":18,"distance":100,"tags":["Multi-Agent","ToM"],"correspondenceRatio":0.3333333333333333},{"id":"l97","source":5,"target":18,"distance":100,"tags":["Multi-Agent","ToM"],"correspondenceRatio":0.3333333333333333},{"id":"l173","source":13,"target":18,"distance":100,"tags":["Multi-Agent","ToM"],"correspondenceRatio":0.3333333333333333},{"id":"l187","source":17,"target":18,"distance":100,"tags":["ToM"],"correspondenceRatio":0.3333333333333333},{"id":"l110","source":6,"target":18,"distance":100,"tags":["ToM"],"correspondenceRatio":0.3333333333333333},{"id":"l108","source":6,"target":16,"distance":100,"tags":["ToM"],"correspondenceRatio":0.3333333333333333},{"id":"l167","source":12,"target":18,"distance":100,"tags":["Multi-Agent"],"correspondenceRatio":0.3333333333333333},{"id":"l166","source":12,"target":17,"distance":100,"tags":["RL"],"correspondenceRatio":0.3333333333333333},{"id":"l185","source":16,"target":18,"distance":100,"tags":["ToM"],"correspondenceRatio":0.3333333333333333},{"id":"l87","source":5,"target":8,"distance":100,"tags":["HCI","ToM"],"correspondenceRatio":0.3333333333333333},{"id":"l83","source":4,"target":18,"distance":100,"tags":["ToM"],"correspondenceRatio":0.3333333333333333},{"id":"l188","source":17,"target":19,"distance":100,"tags":["RL"],"correspondenceRatio":0.3333333333333333},{"id":"l82","source":4,"target":17,"distance":100,"tags":["ToM"],"correspondenceRatio":0.3333333333333333},{"id":"l128","source":8,"target":13,"distance":100,"tags":["HCI","ToM"],"correspondenceRatio":0.3333333333333333},{"id":"l109","source":6,"target":17,"distance":100,"tags":["ToM"],"correspondenceRatio":0.3333333333333333},{"id":"l152","source":10,"target":18,"distance":100,"tags":["Multi-Agent"],"correspondenceRatio":0.3333333333333333},{"id":"l184","source":16,"target":17,"distance":100,"tags":["ToM"],"correspondenceRatio":0.3333333333333333},{"id":"l189","source":18,"target":19,"distance":100,"tags":["Multi-Agent"],"correspondenceRatio":0.3333333333333333},{"id":"l47","source":2,"target":13,"distance":100,"tags":["Games","HCI","Multi-Agent"],"correspondenceRatio":0.42857142857142855},{"id":"l1","source":0,"target":2,"distance":100,"tags":["Games","HCI","Multi-Agent"],"correspondenceRatio":0.42857142857142855},{"id":"l90","source":5,"target":11,"distance":100,"tags":["Games","HCI","Multi-Agent"],"correspondenceRatio":0.42857142857142855},{"id":"l39","source":2,"target":5,"distance":100,"tags":["Games","HCI","Multi-Agent"],"correspondenceRatio":0.42857142857142855},{"id":"l155","source":11,"target":13,"distance":100,"tags":["Games","HCI","Multi-Agent"],"correspondenceRatio":0.42857142857142855},{"id":"l10","source":0,"target":11,"distance":100,"tags":["Games","HCI","Multi-Agent"],"correspondenceRatio":0.42857142857142855},{"id":"l161","source":11,"target":19,"distance":100,"tags":["Multi-Agent","RL"],"correspondenceRatio":0.5},{"id":"l107","source":6,"target":15,"distance":100,"tags":["ToM"],"correspondenceRatio":0.5},{"id":"l44","source":2,"target":10,"distance":100,"tags":["Multi-Agent","RL"],"correspondenceRatio":0.5},{"id":"l154","source":11,"target":12,"distance":100,"tags":["Multi-Agent","RL"],"correspondenceRatio":0.5},{"id":"l145","source":10,"target":11,"distance":100,"tags":["Multi-Agent","RL"],"correspondenceRatio":0.5},{"id":"l56","source":3,"target":6,"distance":100,"tags":["ToM"],"correspondenceRatio":0.5},{"id":"l58","source":3,"target":8,"distance":100,"tags":["ToM"],"correspondenceRatio":0.5},{"id":"l182","source":15,"target":18,"distance":100,"tags":["ToM"],"correspondenceRatio":0.5},{"id":"l46","source":2,"target":12,"distance":100,"tags":["Multi-Agent","RL"],"correspondenceRatio":0.5},{"id":"l181","source":15,"target":17,"distance":100,"tags":["ToM"],"correspondenceRatio":0.5},{"id":"l66","source":3,"target":16,"distance":100,"tags":["ToM"],"correspondenceRatio":0.5},{"id":"l53","source":2,"target":19,"distance":100,"tags":["Multi-Agent","RL"],"correspondenceRatio":0.5},{"id":"l180","source":15,"target":16,"distance":100,"tags":["ToM"],"correspondenceRatio":0.5},{"id":"l67","source":3,"target":17,"distance":100,"tags":["ToM"],"correspondenceRatio":0.5},{"id":"l68","source":3,"target":18,"distance":100,"tags":["ToM"],"correspondenceRatio":0.5},{"id":"l80","source":4,"target":15,"distance":100,"tags":["ToM"],"correspondenceRatio":0.5},{"id":"l130","source":8,"target":15,"distance":100,"tags":["ToM"],"correspondenceRatio":0.5},{"id":"l54","source":3,"target":4,"distance":100,"tags":["ToM"],"correspondenceRatio":0.5},{"id":"l81","source":4,"target":16,"distance":100,"tags":["Emotion","ToM"],"correspondenceRatio":1},{"id":"l12","source":0,"target":13,"distance":100,"tags":["Games","Gaze","HCI","Multi-Agent","ToM","VR"],"correspondenceRatio":1},{"id":"l65","source":3,"target":15,"distance":100,"tags":["ToM"],"correspondenceRatio":1},{"id":"l168","source":12,"target":19,"distance":100,"tags":["Multi-Agent","RL"],"correspondenceRatio":1},{"id":"l4","source":0,"target":5,"distance":100,"tags":["Games","Gaze","HCI","Multi-Agent","ToM","VR"],"correspondenceRatio":1},{"id":"l139","source":9,"target":14,"distance":100,"tags":["Bayesian Inference","Cognitive Science","Food Truck Experiment"],"correspondenceRatio":1},{"id":"l146","source":10,"target":12,"distance":100,"tags":["Multi-Agent","RL"],"correspondenceRatio":1},{"id":"l92","source":5,"target":13,"distance":100,"tags":["Games","Gaze","HCI","Multi-Agent","ToM","VR"],"correspondenceRatio":1},{"id":"l45","source":2,"target":11,"distance":100,"tags":["Games","HCI","Multi-Agent","RL"],"correspondenceRatio":1},{"id":"l153","source":10,"target":19,"distance":100,"tags":["Multi-Agent","RL"],"correspondenceRatio":1},{"id":"l24","source":1,"target":7,"distance":100,"tags":["Autism","Games"],"correspondenceRatio":1}],"nodes":[{"id":"n0","type":"circle","tags":["Games","Gaze","HCI","Multi-Agent","ToM","VR"],"authors":[{"first_name":"Sahil","last_name":"Narang"},{"first_name":"Andrew","last_name":"Best"},{"first_name":"Dinesh","last_name":"Manocha"}],"title":"Inferring User Intent using Bayesian Theory of Mind in Shared Avatar-Agent Virtual Environments","abstract":"We present a real-time algorithm to infer the intention of a user's avatar in a virtual environment shared with multiple human-like agents. Our algorithm applies the Bayesian Theory of Mind approach to make inferences about the avatar's hidden intentions based on the observed proxemics and gaze-based cues. Our approach accounts for the potential irrationality in human behavior, as well as the dynamic nature of an individual's intentions. The inferred intent is used to guide the response of the virtual agent and generate locomotion and gaze-based behaviors. Our overall approach allows the user to actively interact with tens of virtual agents from a first-person perspective in an immersive setting. We systematically evaluate our inference algorithm in controlled multi-agent simulation environments and highlight its ability to reliably and efficiently infer the hidden intent of a user's avatar even under noisy conditions. We quantitatively demonstrate the performance benefits of our approach in terms of reducing false inferences, as compared to a prior method. The results of our user evaluation show that 68.18% of participants reported feeling more comfortable in sharing the virtual environment with agents simulated with our algorithm as compared to a prior inference method, likely as a direct result of significantly fewer false inferences and more plausible responses from the virtual agents.","year":2019},{"id":"n1","type":"circle","tags":["Autism","Games"],"authors":[{"first_name":"Sven","last_name":"Strickroth"},{"first_name":"Dietmar","last_name":"Zoerner"},{"first_name":"Tobias","last_name":"Moebert"},{"first_name":"Anna","last_name":"Morgiel"},{"first_name":"Ulrike","last_name":"Lucke"}],"title":"Game-Based Promotion of Motivation and Attention for Socio-Emotional Training in Autism","abstract":"Caused by a deviance of their reward system, autistic people show attention deficits for learning content outside their special fields of interest. This can lead to significant problems, especially in formal learning situations. A promising approach to increase attention is the use of game-based learning concepts. The effect of individual playful aspects could be shown in existing learning systems. However, these do not provide consistent game experiences, which may result in a decreasing motivation for training. Therefore, this paper presents requirements as well as a related game concept to integrate the learning content with a playful narrative in order to promote motivation and attention for socio-emotional training.","year":2020},{"id":"n2","type":"circle","tags":["Games","HCI","Multi-Agent","RL"],"authors":[{"first_name":"Mark","last_name":"Woodward"},{"first_name":"Chelsea","last_name":"Finn"},{"first_name":"Karol","last_name":"Hausman"}],"title":"Learning to Interactively Learn and Assist","abstract":"When deploying autonomous agents in the real world, we need effective ways of communicating objectives to them. Traditional skill learning has revolved around reinforcement and imitation learning, each with rigid constraints on the format of information exchanged between the human and the agent. While scalar rewards carry little information, demonstrations require significant effort to provide and may carry more information than is necessary. Furthermore, rewards and demonstrations are often defined and collected before training begins, when the human is most uncertain about what information would help the agent. In contrast, when humans communicate objectives with each other, they make use of a large vocabulary of informative behaviors, including non-verbal communication, and often communicate throughout learning, responding to observed behavior. In this way, humans communicate intent with minimal effort. In this paper, we propose such interactive learning as an alternative to reward or demonstration-driven learning. To accomplish this, we introduce a multi-agent training framework that enables an agent to learn from another agent who knows the current task. Through a series of experiments, we demonstrate the emergence of a variety of interactive learning behaviors, including information-sharing, information-seeking, and question-answering. Most importantly, we find that our approach produces an agent that is capable of learning interactively from a human user, without a set of explicit demonstrations or a reward function, and achieving significantly better performance cooperatively with a human than a human performing the task alone.","year":2019},{"id":"n3","type":"circle","tags":["ToM"],"authors":[{"first_name":"Chris L","last_name":"Baker"},{"first_name":"Rebecca","last_name":"Saxe"},{"first_name":"Joshua B","last_name":"Tenenbaum"}],"title":"Action understanding as inverse planning","abstract":"Humans are adept at inferring the mental states underlying other agents’ actions, such as goals, beliefs, desires, emotions and other thoughts. We propose a computational framework based on Bayesian inverse planning for modeling human action understanding. The framework represents an intuitive theory of intentional agents’ behavior based on the principle of rationality: the expectation that agents will plan approximately rationally to achieve their goals, given their beliefs about the world. The mental states that caused an agent's behavior are inferred by inverting this model of rational planning using Bayesian inference, integrating the likelihood of the observed actions with the prior over mental states. This approach formalizes in precise probabilistic terms the essence of previous qualitative approaches to action understanding based on an “intentional stance” [Dennett, D. C. (1987). The intentional stance. Cambridge, MA: MIT Press] or a “teleological stance” [Gergely, G., Nádasdy, Z., Csibra, G., & Biró, S. (1995). Taking the intentional stance at 12 months of age. Cognition, 56, 165–193]. In three psychophysical experiments using animated stimuli of agents moving in simple mazes, we assess how well different inverse planning models based on different goal priors can predict human goal inferences. The results provide quantitative evidence for an approximately rational inference mechanism in human goal inference within our simplified stimulus paradigm, and for the flexible nature of goal representations that human observers can adopt. We discuss the implications of our experimental results for human action understanding in real-world contexts, and suggest how our framework might be extended to capture other kinds of mental state inferences, such as inferences about beliefs, or inferring whether an entity is an intentional agent.","year":2009},{"id":"n4","type":"circle","tags":["Emotion","ToM"],"authors":[{"first_name":"Yang","last_name":"Wu"},{"first_name":"Chris L","last_name":"Baker"},{"first_name":"Joshua B","last_name":"Tenenbaum"},{"first_name":"Laura E","last_name":"Schulz"}],"title":"Rational Inference of Beliefs and Desires From Emotional Expressions","abstract":"We investigated people's ability to infer others’ mental states from their emotional reactions, manipulating whether agents wanted, expected, and caused an outcome. Participants recovered agents’ desires throughout. When the agent observed, but did not cause the outcome, participants’ ability to recover the agent's beliefs depended on the evidence they got (i.e., her reaction only to the actual outcome or to both the expected and actual outcomes; Experiments 1 and 2). When the agent caused the event, participants’ judgments also depended on the probability of the action (Experiments 3 and 4); when actions were improbable given the mental states, people failed to recover the agent's beliefs even when they saw her react to both the anticipated and actual outcomes. A Bayesian model captured human performance throughout (rs ≥ .95), consistent with the proposal that people rationally integrate information about others’ actions and emotional reactions to infer their unobservable mental states.","year":2018},{"id":"n5","type":"circle","tags":["Games","Gaze","HCI","Multi-Agent","ToM","VR"],"authors":[{"first_name":"Sahil","last_name":"Narang"},{"first_name":"Andrew","last_name":"Best"},{"first_name":"Dinesh","last_name":"Manocha"}],"title":"Inferring User Intent using Bayesian Theory of Mind in Shared Avatar-Agent Virtual Environments","abstract":"We present a real-time algorithm to infer the intention of a user's avatar in a virtual environment shared with multiple human-like agents. Our algorithm applies the Bayesian Theory of Mind approach to make inferences about the avatar's hidden intentions based on the observed proxemics and gaze-based cues. Our approach accounts for the potential irrationality in human behavior, as well as the dynamic nature of an individual's intentions. The inferred intent is used to guide the response of the virtual agent and generate locomotion and gaze-based behaviors. Our overall approach allows the user to actively interact with tens of virtual agents from a first-person perspective in an immersive setting. We systematically evaluate our inference algorithm in controlled multi-agent simulation environments and highlight its ability to reliably and efficiently infer the hidden intent of a user's avatar even under noisy conditions. We quantitatively demonstrate the performance benefits of our approach in terms of reducing false inferences, as compared to a prior method. The results of our user evaluation show that 68.18% of participants reported feeling more comfortable in sharing the virtual environment with agents simulated with our algorithm as compared to a prior inference method, likely as a direct result of significantly fewer false inferences and more plausible responses from the virtual agents.","year":2019},{"id":"n6","type":"circle","tags":["Robotics","ToM"],"authors":[{"first_name":"Alan F T","last_name":"Winfield"}],"title":"Experiments in Artificial Theory of Mind: From Safety to Story-Telling","abstract":"Theory of mind is the term given by philosophers and psychologists for the ability to form a predictive model of self and others. In this paper we focus on synthetic models of theory of mind. We contend firstly that such models—especially when tested experimentally—can provide useful insights into cognition, and secondly that artificial theory of mind can provide intelligent robots with powerful new capabilities, in particular social intelligence for human-robot interaction. This paper advances the hypothesis that simulation-based internal models offer a powerful and realisable, theory-driven basis for artificial theory of mind. Proposed as a computational model of the simulation theory of mind, our simulation-based internal model equips a robot with an internal model of itself and its environment, including other dynamic actors, which can test (i.e., simulate) the robot's next possible actions and hence anticipate the likely consequences of those actions both for itself and others. Although it falls far short of a full artificial theory of mind, our model does allow us to test several interesting scenarios: in some of these a robot equipped with the internal model interacts with other robots without an internal model, but acting as proxy humans; in others two robots each with a simulation-based internal model interact with each other. We outline a series of experiments which each demonstrate some aspect of artificial theory of mind.","year":2018},{"id":"n7","type":"circle","tags":["Autism","Games"],"authors":[{"first_name":"Sven","last_name":"Strickroth"},{"first_name":"Dietmar","last_name":"Zoerner"},{"first_name":"Tobias","last_name":"Moebert"},{"first_name":"Anna","last_name":"Morgiel"},{"first_name":"Ulrike","last_name":"Lucke"}],"title":"Game-Based Promotion of Motivation and Attention for Socio-Emotional Training in Autism","abstract":"Caused by a deviance of their reward system, autistic people show attention deficits for learning content outside their special fields of interest. This can lead to significant problems, especially in formal learning situations. A promising approach to increase attention is the use of game-based learning concepts. The effect of individual playful aspects could be shown in existing learning systems. However, these do not provide consistent game experiences, which may result in a decreasing motivation for training. Therefore, this paper presents requirements as well as a related game concept to integrate the learning content with a playful narrative in order to promote motivation and attention for socio-emotional training.","year":2020},{"id":"n8","type":"circle","tags":["HCI","ToM"],"authors":[{"first_name":"Arjun R","last_name":"Akula"},{"first_name":"Changsong","last_name":"Liu"},{"first_name":"Sari","last_name":"Saba-Sadiya"},{"first_name":"Hongjing","last_name":"Lu"},{"first_name":"Sinisa","last_name":"Todorovic"},{"first_name":"Joyce Y","last_name":"Chai"},{"first_name":"Song-Chun","last_name":"Zhu"}],"title":"X-ToM: Explaining with Theory-of-Mind for Gaining Justified Human Trust","abstract":"We present a new explainable AI (XAI) framework aimed at increasing justified human trust and reliance in the AI machine through explanations. We pose explanation as an iterative communication process, i.e. dialog, between the machine and human user. More concretely, the machine generates sequence of explanations in a dialog which takes into account three important aspects at each dialog turn: (a) human's intention (or curiosity); (b) human's understanding of the machine; and (c) machine's understanding of the human user. To do this, we use Theory of Mind (ToM) which helps us in explicitly modeling human's intention, machine's mind as inferred by the human as well as human's mind as inferred by the machine. In other words, these explicit mental representations in ToM are incorporated to learn an optimal explanation policy that takes into account human's perception and beliefs. Furthermore, we also show that ToM facilitates in quantitatively measuring justified human trust in the machine by comparing all the three mental representations. We applied our framework to three visual recognition tasks, namely, image classification, action recognition, and human body pose estimation. We argue that our ToM based explanations are practical and more natural for both expert and non-expert users to understand the internal workings of complex machine learning models. To the best of our knowledge, this is the first work to derive explanations using ToM. Extensive human study experiments verify our hypotheses, showing that the proposed explanations significantly outperform the state-of-the-art XAI methods in terms of all the standard quantitative and qualitative XAI evaluation metrics including human trust, reliance, and explanation satisfaction.","year":2019},{"id":"n9","type":"circle","tags":["Bayesian Inference","Cognitive Science","Food Truck Experiment"],"authors":[{"first_name":"Chris L.","last_name":"Baker"},{"first_name":"Rebecca R.","last_name":"Saxe"},{"first_name":"Joshua B.","last_name":"Tenenbaum"}],"title":"Bayesian Theory of Mind: Modeling Joint Belief-Desire Attribution","year":2011},{"id":"n10","type":"circle","tags":["Multi-Agent","RL"],"authors":[{"first_name":"Pablo","last_name":"Hernandez-Leal"},{"first_name":"Bilal","last_name":"Kartal"},{"first_name":"Matthew E","last_name":"Taylor"}],"title":"Is multiagent deep reinforcement learning the answer or the question? A brief survey","abstract":"Deep reinforcement learning (RL) has achieved outstanding results in recent years. This has led to a dramatic increase in the number of applications and methods. Recent works have explored learning beyond single-agent scenarios and have considered multiagent learning (MAL) scenarios. Initial results report successes in complex multiagent domains, although there are several challenges to be addressed. The primary goal of this article is to provide a clear overview of current multiagent deep reinforcement learning (MDRL) literature. Additionally, we complement the overview with a broader analysis: (i) we revisit previous key components, originally presented in MAL and RL, and highlight how they have been adapted to multiagent deep reinforcement learning settings. (ii) We provide general guidelines to new practitioners in the area: describing lessons learned from MDRL works, pointing to recent benchmarks, and outlining open avenues of research. (iii) We take a more critical tone raising practical challenges of MDRL (e.g., implementation and computational demands). We expect this article will help unify and motivate future research to take advantage of the abundant literature that exists (e.g., RL and MAL) in a joint effort to promote fruitful research in the multiagent community.","year":2018},{"id":"n11","type":"circle","tags":["Games","HCI","Multi-Agent","RL"],"authors":[{"first_name":"Mark","last_name":"Woodward"},{"first_name":"Chelsea","last_name":"Finn"},{"first_name":"Karol","last_name":"Hausman"}],"title":"Learning to Interactively Learn and Assist","abstract":"When deploying autonomous agents in the real world, we need effective ways of communicating objectives to them. Traditional skill learning has revolved around reinforcement and imitation learning, each with rigid constraints on the format of information exchanged between the human and the agent. While scalar rewards carry little information, demonstrations require significant effort to provide and may carry more information than is necessary. Furthermore, rewards and demonstrations are often defined and collected before training begins, when the human is most uncertain about what information would help the agent. In contrast, when humans communicate objectives with each other, they make use of a large vocabulary of informative behaviors, including non-verbal communication, and often communicate throughout learning, responding to observed behavior. In this way, humans communicate intent with minimal effort. In this paper, we propose such interactive learning as an alternative to reward or demonstration-driven learning. To accomplish this, we introduce a multi-agent training framework that enables an agent to learn from another agent who knows the current task. Through a series of experiments, we demonstrate the emergence of a variety of interactive learning behaviors, including information-sharing, information-seeking, and question-answering. Most importantly, we find that our approach produces an agent that is capable of learning interactively from a human user, without a set of explicit demonstrations or a reward function, and achieving significantly better performance cooperatively with a human than a human performing the task alone.","year":2019},{"id":"n12","type":"circle","tags":["Multi-Agent","RL"],"authors":[{"first_name":"Ying","last_name":"Wen"},{"first_name":"Yaodong","last_name":"Yang"},{"first_name":"Rui","last_name":"Luo"},{"first_name":"Jun","last_name":"Wang"},{"first_name":"Wei","last_name":"Pan"}],"title":"Probabilistic Recursive Reasoning for Multi-Agent Reinforcement Learning","abstract":"Humans are capable of attributing latent mental contents such as beliefs or intentions to others. The social skill is critical in daily life for reasoning about the potential consequences of others' behaviors so as to plan ahead. It is known that humans use such reasoning ability recursively by considering what others believe about their own beliefs. In this paper, we start from level-1 recursion and introduce a probabilistic recursive reasoning (PR2) framework for multi-agent reinforcement learning. Our hypothesis is that it is beneficial for each agent to account for how the opponents would react to its future behaviors. Under the PR2 framework, we adopt variational Bayes methods to approximate the opponents' conditional policies, to which each agent finds the best response and then improve their own policies. We develop decentralized-training-decentralized-execution algorithms, namely PR2-Q and PR2-Actor-Critic, that are proved to converge in the self-play scenarios when there exists one Nash equilibrium. Our methods are tested on both the matrix game and the differential game, which have a non-trivial equilibrium where common gradient-based methods fail to converge. Our experiments show that it is critical to reason about how the opponents believe about what the agent believes. We expect our work to contribute a new idea of modeling the opponents to the multi-agent reinforcement learning community.","year":2019},{"id":"n13","type":"circle","tags":["Games","Gaze","HCI","Multi-Agent","ToM","VR"],"authors":[{"first_name":"Sahil","last_name":"Narang"},{"first_name":"Andrew","last_name":"Best"},{"first_name":"Dinesh","last_name":"Manocha"}],"title":"Inferring User Intent using Bayesian Theory of Mind in Shared Avatar-Agent Virtual Environments","abstract":"We present a real-time algorithm to infer the intention of a user's avatar in a virtual environment shared with multiple human-like agents. Our algorithm applies the Bayesian Theory of Mind approach to make inferences about the avatar's hidden intentions based on the observed proxemics and gaze-based cues. Our approach accounts for the potential irrationality in human behavior, as well as the dynamic nature of an individual's intentions. The inferred intent is used to guide the response of the virtual agent and generate locomotion and gaze-based behaviors. Our overall approach allows the user to actively interact with tens of virtual agents from a first-person perspective in an immersive setting. We systematically evaluate our inference algorithm in controlled multi-agent simulation environments and highlight its ability to reliably and efficiently infer the hidden intent of a user's avatar even under noisy conditions. We quantitatively demonstrate the performance benefits of our approach in terms of reducing false inferences, as compared to a prior method. The results of our user evaluation show that 68.18% of participants reported feeling more comfortable in sharing the virtual environment with agents simulated with our algorithm as compared to a prior inference method, likely as a direct result of significantly fewer false inferences and more plausible responses from the virtual agents.","year":2019},{"id":"n14","type":"circle","tags":["Bayesian Inference","Cognitive Science","Food Truck Experiment"],"authors":[{"first_name":"Chris L.","last_name":"Baker"},{"first_name":"Rebecca R.","last_name":"Saxe"},{"first_name":"Joshua B.","last_name":"Tenenbaum"}],"title":"Bayesian Theory of Mind: Modeling Joint Belief-Desire Attribution","year":2011},{"id":"n15","type":"circle","tags":["ToM"],"authors":[{"first_name":"Chris L","last_name":"Baker"},{"first_name":"Julian","last_name":"Jara-Ettinger"},{"first_name":"Rebecca","last_name":"Saxe"},{"first_name":"Joshua B","last_name":"Tenenbaum"}],"title":"Rational quantitative attribution of beliefs, desires and percepts in human mentalizing","abstract":"Social cognition depends on our capacity for mentalizing, or explaining an agent's behaviour in terms of their mental states. The development and neural substrates of mentalizing are well-studied, but its computational basis is only beginning to be probed. Here we present a model of core mentalizing computations: Inferring jointly an actor's beliefs, desires and percepts from how they move in the local spatial environment. Our Bayesian theory of mind (BToM) model is based on probabilistically inverting artificial-intelligence approaches to rational planning and state estimation, which extend classical expected-utility agent models to sequential actions in complex, partially observable domains. The model accurately captures the quantitative mental-state judgements of human participants in two experiments, each varying multiple stimulus dimensions across a large number of stimuli. Comparative model fits with both simpler 'lesioned' BToM models and a family of simpler non-mentalistic motion features reveal the value contributed by each component of our model.","year":2017},{"id":"n16","type":"circle","tags":["Emotion","ToM"],"authors":[{"first_name":"Desmond C.","last_name":"Ong"},{"first_name":"Jamil","last_name":"Zaki"},{"first_name":"Noah D.","last_name":"Goodman"}],"title":"Computational Models of Emotion Inference in Theory of Mind: A Review and Roadmap","abstract":"Research on social cognition has fruitfully applied computational modeling approaches to explain how observers understand and reason about others’ mental states. By contrast, there has been less work on modeling observers’ understanding of emotional states. We propose an intuitive theory framework to studying affective cognition—how humans reason about emotions—and derive a taxonomy of inferences within affective cognition. Using this taxonomy, we review formal computational modeling work on such inferences, including causal reasoning about how others react to events, reasoning about unseen causes of emotions, reasoning with multiple cues, as well as reasoning from emotions to other mental states. In addition, we provide a roadmap for future research by charting out inferences—such as hypothetical and counterfactual reasoning about emotions—that are ripe for future computational modeling work. This framework proposes unifying these various types of reasoning as Bayesian inference within a common “intuitive Theory of Emotion.” Finally, we end with a discussion of important theoretical and methodological challenges that lie ahead in modeling affective cognition.","year":2019},{"id":"n17","type":"circle","tags":["RL","ToM"],"authors":[{"first_name":"Neil C.","last_name":"Rabinowitz"},{"first_name":"Frank","last_name":"Perbet"},{"first_name":"H. Francis","last_name":"Song"},{"first_name":"Chiyuan","last_name":"Zhang"},{"first_name":"Matthew","last_name":"Botvinick"}],"title":"Machine Theory of mind","abstract":"Theory of mind (ToM) broadly refers to humans' ability to represent the mental states of others, including their desires, beliefs, and intentions. We design a Theory of Mind neural network - a ToMnet - which uses meta-learning to build such models of the agents it encounters. The ToMnet learns a strong prior model for agents' future behaviour, and, using only a small number of behavioural observations, can bootstrap to richer predictions about agents' characteristics and mental states. We apply the ToMnet to agents behaving in simple gridworld environments, showing that it learns to model random, algorithmic, and deep RL agents from varied populations, and that it passes classic ToM tasks such as the \"Sally-Anne\" test of recognising that others can hold false beliefs about the world.","year":2018},{"id":"n18","type":"circle","tags":["Multi-Agent","ToM"],"authors":[{"first_name":"Michael","last_name":"Shum"},{"first_name":"Max","last_name":"Kleiman-Weiner"},{"first_name":"Michael L.","last_name":"Littman"},{"first_name":"Joshua B.","last_name":"Tenenbaum"}],"title":"Theory of Minds: Understanding Behavior in Groups through Inverse Planning","abstract":"Human social behavior is structured by relationships. We form teams, groups, tribes, and alliances at all scales of human life. These structures guide multi-agent cooperation and competition, but when we observe others these underlying relationships are typically unobservable and hence must be inferred. Humans make these inferences intuitively and flexibly, often making rapid generalizations about the latent relationships that underlie behavior from just sparse and noisy observations. Rapid and accurate inferences are important for determining who to cooperate with, who to compete with, and how to cooperate in order to compete. Towards the goal of building machine-learning algorithms with human-like social intelligence, we develop a generative model of multiagent action understanding based on a novel representation for these latent relationships called Composable Team Hierarchies (CTH). This representation is grounded in the formalism of stochastic games and multi-agent reinforcement learning. We use CTH as a target for Bayesian inference yielding a new algorithm for understanding behavior in groups that can both infer hidden relationships as well as predict future actions for multiple agents interacting together. Our algorithm rapidly recovers an underlying causal model of how agents relate in spatial stochastic games from just a few observations. The patterns of inference made by this algorithm closely correspond with human judgments and the algorithm makes the same rapid generalizations that people do.","year":2019},{"id":"n19","type":"circle","tags":["Multi-Agent","RL"],"authors":[{"first_name":"Natasha","last_name":"Jaques"},{"first_name":"Angeliki","last_name":"Lazaridou"},{"first_name":"Edward","last_name":"Hughes"},{"first_name":"Caglar","last_name":"Gulcehre"},{"first_name":"Pedro A","last_name":"Ortega"},{"first_name":"D J","last_name":"Strouse"},{"first_name":"Joel Z","last_name":"Leibo"},{"first_name":"Nando","last_name":"de Freitas"}],"title":"Social Influence as Intrinsic Motivation for Multi-Agent Deep Reinforcement Learning","abstract":"We propose a unified mechanism for achieving coordination and communication in Multi-Agent Reinforcement Learning (MARL), through rewarding agents for having causal influence over other agents' actions. Causal influence is assessed using counterfactual reasoning. At each timestep, an agent simulates alternate actions that it could have taken, and computes their effect on the behavior of other agents. Actions that lead to bigger changes in other agents' behavior are considered influential and are rewarded. We show that this is equivalent to rewarding agents for having high mutual information between their actions. Empirical results demonstrate that influence leads to enhanced coordination and communication in challenging social dilemma environments, dramatically increasing the learning curves of the deep RL agents, and leading to more meaningful learned communication protocols. The influence rewards for all agents can be computed in a decentralized way by enabling agents to learn a model of other agents using deep neural networks. In contrast, key previous works on emergent communication in the MARL setting were unable to learn diverse policies in a decentralized manner and had to resort to centralized training. Consequently, the influence reward opens up a window of new opportunities for research in this area.","year":2018}],"directed":false,"multigraph":false}